{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e744312a",
   "metadata": {},
   "source": [
    "# Quixo\n",
    "---\n",
    "<p align=\"center\">\n",
    "<img style=\"float:center\" src=\"../images/quixo.jpg\" alt=\"drawing\" width=\"300\" height=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc39b12bb2717018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:00:13.805873800Z",
     "start_time": "2024-01-16T13:00:13.502711400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint, random, choice\n",
    "from game import Move, Player\n",
    "from CustomGameClass import Quixo as Game\n",
    "from tqdm import trange\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc888e539ec8455",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reinforcement Learning Player\n",
    "---\n",
    "For the implementation of our Reinforcement Learning player, we used the same strategy as for Laboratory 4 i.e. using a Temporal difference method. However, I want to write the entire description of the method for completeness.\n",
    "\n",
    "This class represents a player that uses Reinforcement Learning to make decisions in Quixo. More precisely, our player uses [Temporal difference (TD) learning](https://it.wikipedia.org/wiki/Temporal_difference_learning). TD in reinforcement learning is an unsupervised learning technique very commonly used in it for the purpose of predicting the total reward expected over the future. Essentially, TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods perform state value function updates based on current estimates.\n",
    "\n",
    "Attributes:\n",
    "- ``epochs`` (int): The number of training epochs.\n",
    "- ``alpha`` (float): The learning rate.\n",
    "- ``discount_factor`` (float): The discount factor of the Bellman equation.\n",
    "- ``min_exploration_rate`` (float): The minimum exploration rate during training.\n",
    "- ``exploration_decay_rate`` (float): The rate at which the exploration rate decays during training.\n",
    "- ``opponent`` (Player): The opponent player.\n",
    "- ``states`` (list): A list to store the states visited during a game.\n",
    "- ``state_value`` (dict): A dictionary to store the value of each state.\n",
    "- ``training_phase`` (bool): A boolean to indicate if the player is training or not. It basically enables the exploration if it is true, otherwise it uses only the `state_value` dictionary to make decisions.\n",
    "\n",
    "I want to underline that I saw the idea of ``exploration_decay_rate`` from [Davide Sferrazza](https://github.com/FarInHeight/Computational-Intelligence/tree/main/lab10) in his Lab 4 implementation thanks to the peer review.\n",
    "\n",
    "Methods:\n",
    "- ``give_rew(reward)``: Placeholder method for giving reward to the player.\n",
    "- ``add_state(state)``: Adds to the ``states`` array the state that a player has seen during a game.\n",
    "- ``reset()``: Reset the ``states`` array to be able to start a new game.\n",
    "- ``make_move(game)``: Chooses an action to take based on the current game state that can be random or based on the value of the dictionary. It takes from the dictionary, for each possible move, the value associated with the state of the board with the move performed. The maximum value will be the move to execute. We use the following recursive (bellman equation) formula to compute the state-value table: \n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha * (\\gamma * V(S_t +1) - V(S_t))\n",
    "$$\n",
    "The formula simply tells us that the updated value of state t equals the current value of state t adding the difference between the value of the next state , which is multiplied by the discount factor of the Bellman Equation, and the value of the current state, which is multiplied by a learning rate α. The logic is that we update the current value slowly based on our latest observation.\n",
    "- ``update_state_value_table(reward)``: Updates the values of the ``states_value`` dictionary based on the states that the player has seen during the game and the reward that they have provided.\n",
    "- ``game_reward(player)``: Calculates the reward for the player in the current game.\n",
    "- ``train()``: Trains the player using reinforcement learning.\n",
    "- ``save_policy(name)``: Saves the state value table to a file.\n",
    "- ``load_policy(file)``: Loads the state value table from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f3f43236fc839f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:00:13.817686Z",
     "start_time": "2024-01-16T13:00:13.810873Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RLPlayer(Player):\n",
    "    def __init__(self, epochs: int,\n",
    "                 alpha: float,\n",
    "                 discount_factor: float,\n",
    "                 min_exploration_rate: float,\n",
    "                 exploration_decay_rate: float,\n",
    "                 opponent: 'Player',\n",
    "                 training_phase: bool) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = 1\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        self.opponent = opponent\n",
    "        self.training_phase = training_phase\n",
    "        self.states=[]\n",
    "        self.state_value = {}\n",
    "    \n",
    "    def add_state(self, state):\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "    \n",
    "    def make_move(self, game: Game) -> tuple[tuple[int, int], Move]:\n",
    "        available_moves = get_possible_moves(game, game.get_current_player())\n",
    "        if self.training_phase and (random() < self.exploration_rate):  # do exploration\n",
    "            return choice(available_moves)\n",
    "        else:  # do exploitation\n",
    "            value_max = -math.inf\n",
    "            for move in available_moves:\n",
    "                tmp = game.get_board()\n",
    "                game._Game__move(move[0], move[1], game.get_current_player())\n",
    "                next_status = convert_matrix_board_to_tuple(game.get_board())\n",
    "                game.set_board(tmp)\n",
    "                value = 0 if self.state_value.get(next_status) is None else self.state_value.get(next_status)\n",
    "                if value > value_max:\n",
    "                    value_max = value\n",
    "                    action = move\n",
    "        return action\n",
    "        \n",
    "    def update_state_value_table(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.state_value.get(st) is None:\n",
    "                self.state_value[st] = 0\n",
    "            current_value = self.state_value[st]\n",
    "            reward = current_value + self.alpha * (self.discount_factor * reward - current_value)\n",
    "            self.state_value[st] = reward\n",
    "        \n",
    "    def game_reward(self, player: 'Player')-> Literal[-10, 10]:\n",
    "        if self == player:\n",
    "            return 10\n",
    "        else:\n",
    "            return -10\n",
    "    \n",
    "    def train(self, player_name='') -> None:\n",
    "        game = Game()\n",
    "        all_rewards = []\n",
    "        # define how many episodes to run\n",
    "        pbar = trange(self.epochs)\n",
    "        # define the players\n",
    "        players = (self, self.opponent)\n",
    "        \n",
    "        for epochs in pbar:\n",
    "            rewards = 0\n",
    "            winner = -1\n",
    "            players = (players[1], players[0])\n",
    "            player_idx = 1\n",
    "            \n",
    "            while winner < 0:\n",
    "                # change player\n",
    "                player_idx = (player_idx + 1) % 2\n",
    "                player = players[player_idx]\n",
    "                game.switch_player()\n",
    "                \n",
    "                ok = False\n",
    "                if self == player:\n",
    "                    while not ok:\n",
    "                        from_pos, slide = self.make_move(game)\n",
    "                        ok = game._Game__move(from_pos, slide, game.get_current_player())\n",
    "                        state_after_move = convert_matrix_board_to_tuple(game.get_board())\n",
    "                        self.add_state(state_after_move)\n",
    "                        \n",
    "                else:\n",
    "                    while not ok:\n",
    "                        from_pos, slide = player.make_move(game)\n",
    "                        ok = game._Game__move(from_pos, slide, game.get_current_player())\n",
    "                        \n",
    "                winner = game.check_winner()\n",
    "            \n",
    "            # update the exploration rate\n",
    "            self.exploration_rate = np.clip(\n",
    "                np.exp(-self.exploration_decay_rate * epochs), self.min_exploration_rate, 1\n",
    "            )\n",
    "            \n",
    "            reward = self.game_reward(player)\n",
    "            self.update_state_value_table(reward)\n",
    "            rewards += reward\n",
    "            all_rewards.append(rewards)\n",
    "            \n",
    "            self.reset()\n",
    "            game.reset()\n",
    "            \n",
    "            pbar.set_description(f'rewards value: {rewards}, current exploration rate: {self.exploration_rate:2f}')\n",
    "        \n",
    "        plot_training_trends(all_rewards, filename=f\"{player_name} trained against {self.opponent.__class__.__name__}\")\n",
    "        \n",
    "        print(f'** Last 50_000 episodes - Mean rewards value: {sum(all_rewards[-50_000:]) / 50_000:.2f} **')\n",
    "\n",
    "    def save_policy(self, name):\n",
    "        fw = open(name, 'wb')\n",
    "        pickle.dump(self.state_value, fw, protocol=4)\n",
    "        fw.close()\n",
    "\n",
    "    def load_policy(self, file):\n",
    "        fr = open(file, 'rb')\n",
    "        self.state_value = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5563081a6eef8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MinMax Player\n",
    "---\n",
    "\n",
    "This class represents a player who uses the MinMax algorithm to make decisions in the game. The MinMax algorithm is a search algorithm that is used in two-player games to make optimal decisions.\n",
    "\n",
    "Attributes:\n",
    "- ``playerPlaying`` (int): The player who is currently playing.\n",
    "- ``levels_depth`` (list): A list of tuples, where each tuple contains the depth level and the maximum number of possible moves for that depth level.\n",
    "\n",
    "Methods:\n",
    "- ``game_evaluation``: Evaluate the game state based on the current player and depth. If player X wins, the reward is 1, otherwise it is -1. If player O plays instead, the rewards are reversed.\n",
    "- ``min_max``: Perform the Minimax algorithm to determine the best move for the current player. This method takes as input the current game, the alpha and beta values (used for alpha-beta pruning, a technique for reducing the number of nodes evaluated by the MinMax algorithm), and the current depth of the search tree. If the depth is zero or if there is a winner in the game, the method returns the game rating and no moves. Otherwise, for each possible move, it creates a copy of the game, makes the move, and recursively calls the min_max method on the copy of the game. If the returned rating is greater than alpha, alpha is updated and the move is considered the best move. If beta is less than or equal to alpha, the cycle stops for alpha-beta pruning.\n",
    "The process is similar for the case where the current player is non-zero, with the difference that we try to minimize the rating instead of maximizing it.\n",
    "- ``choose_action``: Choose the best action (move) for the current player using the Minimax algorithm. The method starts by getting all possible moves for the current player using the `get_possible_moves` function. It then calculates the search depth for the MinMax algorithm based on the number of possible moves. This is done through a for loop that passes through the `levels_Depth` list. If the number of possible moves is greater than a certain value, the depth is set to a certain level. The cycle stops as soon as a level is found that does not exceed the number of possible moves. The method then calls the `min_max` method to determine the best move for the current player. Finally, the method returns the best move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508616c445755cc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:00:13.829345500Z",
     "start_time": "2024-01-16T13:00:13.818685700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MinMaxPlayer(Player):\n",
    "    def __init__(self, playerPlaying, levels_depth):\n",
    "        super().__init__()\n",
    "        self.moves_value = []\n",
    "        self.playerPlaying = playerPlaying\n",
    "        self.levels_depth = levels_depth\n",
    "\n",
    "    def game_evaluation(self, game: Game, depth):\n",
    "        win = game.check_winner()\n",
    "        ret = 0 + depth\n",
    "        if win == 0 and self.playerPlaying == 0:\n",
    "            ret = 100 + depth\n",
    "        elif win == 0 and self.playerPlaying == 1:\n",
    "            ret = -100 - depth\n",
    "        elif win == 1 and self.playerPlaying == 1:\n",
    "            ret = 100 + depth\n",
    "        elif win == 1 and self.playerPlaying == 0:\n",
    "            ret = - 100 - depth\n",
    "        return ret\n",
    "\n",
    "    def min_max(self, game: 'Game', alpha, beta, depth):\n",
    "        if depth <= 0 or game.check_winner() != -1:\n",
    "            return self.game_evaluation(game, depth), None\n",
    "        best_move = None\n",
    "        if game.current_player == self.playerPlaying:\n",
    "            for move in get_possible_moves(game, game.get_current_player()):\n",
    "                tmp = game.get_board()\n",
    "                g = Game()\n",
    "                g.set_board(tmp)\n",
    "                g.current_player = self.playerPlaying\n",
    "                g._Game__move(move[0], move[1], g.get_current_player())\n",
    "                g.current_player = 1 - self.playerPlaying\n",
    "                eval, _ = self.min_max(g, alpha, beta, depth - 1)\n",
    "                if eval > alpha:\n",
    "                    alpha = eval\n",
    "                    best_move = move\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return alpha, best_move\n",
    "        else:\n",
    "            for move in get_possible_moves(game, game.get_current_player()):\n",
    "                tmp = game.get_board()\n",
    "                g = Game()\n",
    "                g.set_board(tmp)\n",
    "                g.current_player = 1 - self.playerPlaying\n",
    "                g._Game__move(move[0], move[1], g.get_current_player())\n",
    "                g.current_player = self.playerPlaying\n",
    "                eval, _ = self.min_max(g, alpha, beta, depth - 1)\n",
    "                if eval < beta:\n",
    "                    beta = eval\n",
    "                    best_move = move\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return beta, best_move\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        possibleMoves = get_possible_moves(game, game.get_current_player())\n",
    "        possibleMoveCount = len(possibleMoves)\n",
    "\n",
    "        depth = 0\n",
    "        for depthLvl in self.levels_depth:\n",
    "            if possibleMoveCount > depthLvl[1]:\n",
    "                depth = depthLvl[0]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        _, move = self.min_max(game, -math.inf, math.inf, depth)\n",
    "\n",
    "        return move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372aa8b26ec52f69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Utility functions\n",
    "---\n",
    "\n",
    "- ``convert_matrix_to_tuple``: Convert the matrix board representation to a tuple representation.\n",
    "- ``get_possible_moves``: Returns a list of possible moves for the player. It returns a list of tuples, where each tuple contains the move coordinates and the move direction. Each move is a tuple of the form (row, column, direction). The direction is a string that can be either 'up', 'down', 'left' or 'right'.\n",
    "- `test_player`: Test the performance of a player against another player. The method takes as input the two players, the number of games to play, and the name of the two players just to specify them in the plot. The method prints the number of wins for the first player and the number of wins for the second player.\n",
    "- ``plot_total_win_rate``: Creates a bar graph to display the total number of wins of two players in a game. The generated image is saved in the ``images`` folder.\n",
    "- ``plot_training_trends``: Creates a graph to visualize the trend of total rewards while training a reinforcement learning agent. Within the function, the average of the rewards is calculated every 500 training steps. The generated image is saved in the ``images`` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ee66971b395087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:00:13.851722300Z",
     "start_time": "2024-01-16T13:00:13.822345600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_matrix_board_to_tuple(board):\n",
    "    \"\"\"\n",
    "    Converts a matrix board to a tuple representation.\n",
    "\n",
    "    Args:\n",
    "        board (list): The matrix board to be converted.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The converted tuple representation of the board.\n",
    "    \"\"\"\n",
    "    current_board = tuple(tuple(riga) for riga in board)\n",
    "    return current_board\n",
    "\n",
    "def get_possible_moves(game: 'Game', player: int) -> list[tuple[tuple[int, int], Move]]:\n",
    "    \"\"\"\n",
    "    Get a list of possible moves for a given player in the game.\n",
    "\n",
    "    Args:\n",
    "        game (Game): The game object representing the current state of the game.\n",
    "        player (int): The player for whom to find the possible moves.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[tuple[int, int], Move]]: A list of tuples, where each tuple contains the coordinates of a possible move\n",
    "        and the corresponding move direction.\n",
    "\n",
    "    \"\"\"\n",
    "    # possible moves:\n",
    "    # - take border empty and fill the hole by moving in the 3 directions\n",
    "    # - take one of your blocks on the border and fill the hole by moving in the 3 directions\n",
    "    # 44 at start possible moves\n",
    "    pos = set()\n",
    "    for r in [0, 4]:\n",
    "        for c in range(5):\n",
    "            if game.get_board()[r, c] == -1 or game.get_board()[r, c] == player:\n",
    "                if r == 0 and c == 0:  # OK\n",
    "                    pos.add(((c, r), Move.BOTTOM))\n",
    "                    pos.add(((c, r), Move.RIGHT))\n",
    "                elif r == 0 and c == 4:  # OK\n",
    "                    pos.add(((c, r), Move.BOTTOM))\n",
    "                    pos.add(((c, r), Move.LEFT))\n",
    "                elif r == 4 and c == 0:  # OK\n",
    "                    pos.add(((c, r), Move.TOP))\n",
    "                    pos.add(((c, r), Move.RIGHT))\n",
    "                elif r == 4 and c == 4:  # OK\n",
    "                    pos.add(((c, r), Move.TOP))\n",
    "                    pos.add(((c, r), Move.LEFT))\n",
    "                elif r == 0:  # OK\n",
    "                    pos.add(((c, r), Move.BOTTOM))\n",
    "                    pos.add(((c, r), Move.LEFT))\n",
    "                    pos.add(((c, r), Move.RIGHT))\n",
    "                elif r == 4:  # OK\n",
    "                    pos.add(((c, r), Move.TOP))\n",
    "                    pos.add(((c, r), Move.LEFT))\n",
    "                    pos.add(((c, r), Move.RIGHT))\n",
    "    for c in [0, 4]:\n",
    "        for r in range(5):\n",
    "            if game.get_board()[r, c] == -1 or game.get_board()[r, c] == player:\n",
    "                if r == 0 and c == 0:  # OK\n",
    "                    pos.add(((c, r), Move.BOTTOM))\n",
    "                    pos.add(((c, r), Move.RIGHT))\n",
    "                elif r == 0 and c == 4:  # OK\n",
    "                    pos.add(((c, r), Move.BOTTOM))\n",
    "                    pos.add(((c, r), Move.LEFT))\n",
    "                elif r == 4 and c == 0:  # OK\n",
    "                    pos.add(((c, r), Move.TOP))\n",
    "                    pos.add(((c, r), Move.RIGHT))\n",
    "                elif r == 4 and c == 4:  # OK\n",
    "                    pos.add(((c, r), Move.TOP))\n",
    "                    pos.add(((c, r), Move.LEFT))\n",
    "                elif c == 0:\n",
    "                    pos.add(((c, r), Move.TOP))\n",
    "                    pos.add(((c, r), Move.RIGHT))\n",
    "                    pos.add(((c, r), Move.BOTTOM))\n",
    "                elif c == 4:\n",
    "                    pos.add(((c, r), Move.TOP))\n",
    "                    pos.add(((c, r), Move.LEFT))\n",
    "                    pos.add(((c, r), Move.BOTTOM))\n",
    "    return list(pos)\n",
    "\n",
    "def test_player(player1, player2, num_games, name_player1, name_player2):\n",
    "    \"\"\"\n",
    "    Test the performance of two players in a series of games.\n",
    "\n",
    "    Parameters:\n",
    "    player1 (object): The first player object.\n",
    "    player2 (object): The second player object.\n",
    "    num_games (int): The number of games to be played.\n",
    "    name_player1 (str): The name of the first player.\n",
    "    name_player2 (str): The name of the second player.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    g = Game()\n",
    "    player1_wins = 0\n",
    "    player2_wins = 0\n",
    "    draws = 0\n",
    "    games = 0\n",
    "    for _ in range(num_games):\n",
    "        winner = g.play(player1, player2)\n",
    "        games += 1\n",
    "        g.reset()\n",
    "        if winner == 0:\n",
    "            player1_wins += 1\n",
    "        if winner == 1:\n",
    "            player2_wins += 1\n",
    "        if winner == -1:\n",
    "            draws += 1\n",
    "    \n",
    "    plot_total_win_rate(player1_wins, player2_wins, draws, name_player1, name_player2)\n",
    "    print(f\"{name_player1} won {player1_wins / num_games * 100}%\")\n",
    "    print(f\"{name_player2} won {player2_wins / num_games * 100}%\")\n",
    "    print(f\"Draws: {draws / num_games * 100}%\")\n",
    "\n",
    "def plot_total_win_rate(wins_player1, wins_player2, draws, name_player1, name_player2):\n",
    "    \"\"\"\n",
    "    Plots the total win rate of two players.\n",
    "\n",
    "    Parameters:\n",
    "    - wins_player1 (int): Number of wins for player 1.\n",
    "    - wins_player2 (int): Number of wins for player 2.\n",
    "    - draws (int): Number of draws.\n",
    "    - name_player1 (str): Name of player 1.\n",
    "    - name_player2 (str): Name of player 2.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.bar([name_player1, name_player2, 'draws'], [wins_player1, wins_player2, draws], color=['blue', 'orange', 'green'])\n",
    "    plt.ylabel('Number of games')\n",
    "    plt.savefig(f\"images/{name_player1} wins vs {name_player2} wins.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_trends(total_rewards: [int], filename=''):\n",
    "    \"\"\"\n",
    "    Plots the training trends of the total rewards.\n",
    "\n",
    "    Args:\n",
    "        total_rewards (list[int]): List of total rewards obtained during training.\n",
    "        filename (str, optional): Name of the file to save the plot. Defaults to ''.\n",
    "    \"\"\"\n",
    "    mean_array = np.mean(np.array(total_rewards).reshape(-1, 500), axis=1)\n",
    "    index = np.arange(0, len(total_rewards), 500)\n",
    "    plt.plot(index, mean_array, label='Mean rewards value')\n",
    "    plt.ylabel('Mean rewards value')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.savefig(f\"images/{filename} training_trends.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5077d8d",
   "metadata": {},
   "source": [
    "## Random Player Definition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8770ff3f8377b1b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:00:13.852722Z",
     "start_time": "2024-01-16T13:00:13.836361300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        from_pos = (randint(0, 4), randint(0, 4))\n",
    "        move = choice([Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT])\n",
    "        return from_pos, move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99272dd2",
   "metadata": {},
   "source": [
    "## Human Player Definition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8a9c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:00:13.852722Z",
     "start_time": "2024-01-16T13:00:13.841923400Z"
    }
   },
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "            available_moves = get_possible_moves(game, game.get_current_player())\n",
    "            while True:\n",
    "                row = int(input(\"Input your action row:\"))\n",
    "                col = int(input(\"Input your action column:\"))\n",
    "                from_pos = (col, row)\n",
    "                move = int(input(\"Input your action move: (1 for top, 2 for bottom, 3 for left, 4 for right):\"))\n",
    "                if move == 1:\n",
    "                    move = Move.TOP\n",
    "                elif move == 2:\n",
    "                    move = Move.BOTTOM\n",
    "                elif move == 3:\n",
    "                    move = Move.LEFT\n",
    "                elif move == 4:\n",
    "                    move = Move.RIGHT\n",
    "                else:\n",
    "                    print(\"Invalid move, please input again\")\n",
    "                    continue\n",
    "                \n",
    "                if (from_pos, move) in available_moves:\n",
    "                    return from_pos, move\n",
    "                else:\n",
    "                    print(\"Invalid move, please input again\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2415c1",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "---\n",
    "- ``epochs``: training epochs\n",
    "- ``alpha``: learning rate\n",
    "- ``discount_factor``: the discount rate of the Bellman equation\n",
    "- ``min_exploration_rate``: the minimum rate for exploration during the training phase\n",
    "- ``exploration_decay_rate``: the exploration decay rate used during the training\n",
    "- ``training_phase``: a boolean value that indicates if the player is in training phase or not. It basically enables the exploration if it is true, otherwise it uses only the `state_value` dictionary to make decisions.\n",
    "- ``RandomP``: the opponent to play against that use a Random strategy for the RL training\n",
    "- ``MinMaxP``: the opponent to play against that use a MinMax strategy for the RL testing. It takes as input the `level_depth`, a list of tuples, where each tuple contains the depth level and the maximum number of possible moves for that depth level. For example, if there are five possible moves, the depth level is 4, if there are 40 possible moves, the depth level is 1.\n",
    "- ``num_games``: number of games for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543dae82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:00:13.852722Z",
     "start_time": "2024-01-16T13:00:13.845845700Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 500000\n",
    "alpha = 0.1\n",
    "discount_factor = 0.95\n",
    "min_exploration_rate=0.01\n",
    "exploration_decay_rate=5e-6\n",
    "training_phase=True\n",
    "RandomP = RandomPlayer()\n",
    "MinMaxP = MinMaxPlayer(0, [(4,0),(3,23),(2,28),(1,32)])\n",
    "num_games = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362a90d",
   "metadata": {},
   "source": [
    "## Let's do some computation: RL Player trained against Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a19cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the RL player\n",
    "rl_agent_RandomOpponent = RLPlayer(\n",
    "    epochs=epochs,\n",
    "    alpha=alpha,\n",
    "    discount_factor=discount_factor,\n",
    "    min_exploration_rate=min_exploration_rate,\n",
    "    exploration_decay_rate=exploration_decay_rate,\n",
    "    opponent=RandomP,\n",
    "    training_phase=training_phase\n",
    ")\n",
    "# train the RL player\n",
    "rl_agent_RandomOpponent.train(player_name='RL_Player_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f67c53",
   "metadata": {},
   "source": [
    "## Test Reinforcement Learning Player vs Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc05178fcd495a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T12:52:52.537302800Z",
     "start_time": "2024-01-15T12:51:36.181590900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rl_player = RLPlayer(\n",
    "    epochs=epochs,\n",
    "    alpha=alpha,\n",
    "    discount_factor=discount_factor,\n",
    "    min_exploration_rate=min_exploration_rate,\n",
    "    exploration_decay_rate=exploration_decay_rate,\n",
    "    opponent=RandomP,\n",
    "    training_phase=False\n",
    ")\n",
    "\n",
    "rl_player.load_policy('RL_player_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf683719776545aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T12:54:14.553294Z",
     "start_time": "2024-01-15T12:53:57.920723300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_player(rl_player, RandomP, num_games, 'RL_player_1(first_move)', 'Random Player')\n",
    "test_player(RandomP, rl_player, num_games, 'Random Player', 'RL_player_1(second_move)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb8158a7e7b1d4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### RL Player results\n",
    "---\n",
    "\n",
    "- **RL Player 1**\n",
    "  - ``epochs`` = 500_000,\n",
    "  - ``alpha`` = 0.1,\n",
    "  - ``discount_factor`` = 0.95,\n",
    "  - ``min_exploration_rate`` = 0.01,\n",
    "  - ``exploration_decay_rate`` = 1e-5,\n",
    "   \n",
    "  _Results_\n",
    "  - length of ``states_value`` dictionary:  3_988_351\n",
    "  - Last 50000 episodes - Mean rewards value: 6.44\n",
    "  - win rate vs ``RandomPlayer`` in 1000 games (RL always first move) - 89%\n",
    "  -  win rate vs ``RandomPlayer`` in 1000 games (Random always first move) - 75%\n",
    "  \n",
    "\n",
    "- **RL Player 2**\n",
    "  - ``epochs`` = 750_000,\n",
    "  - ``alpha`` = 0.1,\n",
    "  - ``discount_factor`` = 0.95,\n",
    "  - ``min_exploration_rate`` = 0.01,\n",
    "  - ``exploration_decay_rate`` = 5e-6,\n",
    "   \n",
    "  _Results_\n",
    "  - length of ``states_value`` dictionary:  6_610_522\n",
    "  - Last 50000 episodes - Mean rewards value: 5.78\n",
    "  - win rate vs ``RandomPlayer`` in 1000 games (RL always first move) - 88%\n",
    "  -  win rate vs ``RandomPlayer`` in 1000 games (Random always first move) - 63%\n",
    "  \n",
    "\n",
    "- **RL Player 3**\n",
    "  - ``epochs`` = 850_000,\n",
    "  - ``alpha`` = 0.1,\n",
    "  - ``discount_factor`` = 0.95,\n",
    "  - ``min_exploration_rate`` = 0.01,\n",
    "  - ``exploration_decay_rate`` = 5e-6,\n",
    "   \n",
    "  _Results_\n",
    "  - length of ``states_value`` dictionary:  8_727_589\n",
    "  - Last 50000 episodes - Mean rewards value: 5.39\n",
    "  - win rate vs ``RandomPlayer`` in 1000 games (RL always first move) - 83%\n",
    "  -  win rate vs ``RandomPlayer`` in 1000 games (Random always first move) - 67%\n",
    "\n",
    "RL Player 1 is the best policy we obtained, and we saved it in ``RL_player_1``. We will use it for the next tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a08d3",
   "metadata": {},
   "source": [
    "## Test MinMax Player vs Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_player(MinMaxP, RandomP, num_games, 'MinMax Player(first_move)', 'Random Player')\n",
    "test_player(RandomP, MinMaxP, num_games, 'Random Player', 'MinMax Player(second_move)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d91786",
   "metadata": {},
   "source": [
    "## Results and conclusions\n",
    "---\n",
    "\n",
    "We consider a game as a draw if it lasts more than 200 moves (100 moves for player). This is because the game can last forever. In fact, if the two players play optimally, the game will never end. It happens if we play the MinMax player against the RL player but depends on the train run of the Rl player. In fact, we didn't consider testing the two of them against each other because it's irrelevant, either one player always wins, or the other always wins, or they always draw.\n",
    "\n",
    "\n",
    "We can see from the results that all our RL players win a decent number of matches against the Random player, but they can't compete against the MinMax player. Our Rl players see only a little part of the state space and to improve them, we should train them for a lot more epochs. We tried to train a player for two million epochs, but the policy obtained was much larger than that of our players and the results were not too distant. This indicates that to obtain a truly high-performance player, we need to reduce the number of keys in the dictionary using symmetries and train it for a long time. This could certainly be the first improvement to be made in the future.\n",
    "\n",
    "As a conclusion, we can say that the RL player is too much run dependent and the policies that we created are too big, and they do not provide a good result as MinMax. They need also a lot of RAM to play. So, we think that reinforcement Learning is not a good approach for Quixo.\n",
    "\n",
    "### Reinforcement Learning training trends\n",
    "\n",
    "#### RL Player 1 \n",
    "<img src=\"images/RL_Player_1 trained against RandomPlayer training_trends.png\">\n",
    "\n",
    "#### RL Player 2 \n",
    "<img src=\"images/RL_Player_2 trained against RandomPlayer training_trends.png\">\n",
    "\n",
    "#### RL Player 3 \n",
    "<img src=\"images/RL_Player_3 trained against RandomPlayer training_trends.png\">\n",
    "\n",
    "### Reinforcement Learning (RL_player_1) vs Random Player\n",
    "<img src=\"images/RL_player_1(first_move) wins vs Random Player wins.png\">\n",
    "<img src=\"images/Random Player wins vs RL_player_1(second_move) wins.png\">\n",
    "\n",
    "### Reinforcement Learning (RL_player_2) vs Random Player\n",
    "<img src=\"images/RL_player_2(first_move) wins vs Random Player wins.png\">\n",
    "<img src=\"images/Random Player wins vs RL_player_2(second_move) wins.png\">\n",
    "\n",
    "\n",
    "### Reinforcement Learning (RL_player_3) vs Random Player\n",
    "<img src=\"images/RL_player_3(first_move) wins vs Random Player wins.png\">\n",
    "<img src=\"images/Random Player wins vs RL_player_3(second_move) wins.png\">\n",
    "\n",
    "\n",
    "### MinMax Player vs Random Player\n",
    "<img src=\"images/MinMax Player(first_move) wins vs Random Player wins.png\">\n",
    "<img src=\"images/Random Player wins vs MinMax Player(second_move) wins.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14911d53",
   "metadata": {},
   "source": [
    "## Let's play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719c08c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:00:44.574093600Z",
     "start_time": "2024-01-16T13:00:17.167143700Z"
    }
   },
   "outputs": [],
   "source": [
    "''' board indexes\n",
    "\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "  0 1 2 3 4    \n",
    "\n",
    "'''\n",
    "\n",
    "rl_player = RLPlayer(\n",
    "    epochs=epochs,\n",
    "    alpha=alpha,\n",
    "    discount_factor=discount_factor,\n",
    "    min_exploration_rate=min_exploration_rate,\n",
    "    exploration_decay_rate=exploration_decay_rate,\n",
    "    opponent=RandomP,\n",
    "    training_phase=False\n",
    ")\n",
    "rl_player.load_policy('RL_player_1')\n",
    "\n",
    "human_player = HumanPlayer()\n",
    "\n",
    "g = Game()\n",
    "winner = g.play(rl_player, human_player, print_flag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
