{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a77a377d347e62",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tic Tac Toe\n",
    "---\n",
    "\n",
    "<img style=\"float:center\" src=\"../images/tris.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T18:45:31.646292600Z",
     "start_time": "2023-12-25T18:45:31.618779500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23c718aca885aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Board State\n",
    "---\n",
    "The ``TicTacToe`` class reflects the state of the board.\n",
    "We use 1 to indicate player1 and -1 for player 2.\n",
    "\n",
    "#### Parameter description\n",
    "- ``board``: numpy array of dimension 3x3 that represents the game board.\n",
    "- ``players``: contains the numbers that identify our players (1 is ``player1`` and -1 is ``player2``).\n",
    "- ``current_player``: indicates who has to take the turn.\n",
    "- ``winner``: indicates the winner of the game (1 if ``player1`` won, -1 if ``player2`` won).\n",
    "- ``game_over``: boolean that indicate if the game has finished.\n",
    "\n",
    "#### Methods description\n",
    "- ``available_moves``: it returns an array with the list of possible moves (each element is a tuple of two integers that indicates where to play).\n",
    "- ``make_move``: takes as input the location of where a player played and puts the value of the ``current player`` in that place on the board, i.e. who is playing at that moment. It also calls the functions ``check_winner`` and ``switch_player`` to control if that move makes a player win and to and gives the other player the turn. It returns the new board but not as a matrix but as a tuple of tuples using the ``convert_matrix_to_tuple`` function, just to be more comfortable with the QAgent dictionary implementation.\n",
    "- ``switch_player``: It gives the other player the turn. If there is, it set the ``winner`` param with the player who won (1 or -1) and also set ``game_over`` to ``True``.\n",
    "- ``check_winner``: It checks if there is a winner. \n",
    "- ``convert_matrix_to_tuple``: It converts a matrix (that will always be the board status) to a tuple of tuples. For example, a matrix [[1,0,0],[0,1,0],[0,0,1]] will become ((1,0,0), (0,1,0),(0,0,1)).\n",
    "- ``reset``: It resets the state of the board by emptying the boxes with also all the other parameters.\n",
    "- ``show_board``: It prints the board status. ``player1`` is the X and ``player2`` is the O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "abed96533849eaf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T18:45:31.660148100Z",
     "start_time": "2023-12-25T18:45:31.649299200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.players = [1, -1]\n",
    "        self.current_player = 1\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def available_moves(self):\n",
    "        moves = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i][j] == 0:\n",
    "                    moves.append((i, j))\n",
    "        return moves\n",
    "\n",
    "    def make_move(self, move):\n",
    "        if self.board[move[0]][move[1]] != 0:\n",
    "            return False\n",
    "        self.board[move[0]][move[1]] = self.current_player\n",
    "        self.check_winner()\n",
    "        self.switch_player()\n",
    "        return self.convert_matrix_to_tuple(self.board)\n",
    "\n",
    "    def switch_player(self):\n",
    "        if self.current_player == self.players[0]:\n",
    "            self.current_player = self.players[1]\n",
    "        else:\n",
    "            self.current_player = self.players[0]\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows\n",
    "        for i in range(3):\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != 0:\n",
    "                self.winner = self.board[i][0]\n",
    "                self.game_over = True\n",
    "        # Check columns\n",
    "        for j in range(3):\n",
    "            if self.board[0][j] == self.board[1][j] == self.board[2][j] != 0:\n",
    "                self.winner = self.board[0][j]\n",
    "                self.game_over = True\n",
    "        # Check diagonals\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != 0:\n",
    "            self.winner = self.board[0][0]\n",
    "            self.game_over = True\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != 0:\n",
    "            self.winner = self.board[0][2]\n",
    "            self.game_over = True\n",
    "        # Check tie\n",
    "        if len(self.available_moves())==0:\n",
    "            self.winner = 0\n",
    "            self.game_over = True\n",
    "    \n",
    "    def convert_matrix_to_tuple(self, board):\n",
    "        current_board = tuple(tuple(riga) for riga in board)\n",
    "        return current_board\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.current_player = 1\n",
    "        self.winner = 0\n",
    "        self.game_over = False\n",
    "        \n",
    "    def show_board(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, 3):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, 3):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')     \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1f7186f2946cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Q Learning Player\n",
    "---\n",
    "The QLearningAgent represents the player that will be trained with Q-Learning. Q-learning is a reinforcement learning technique which is based on updating the action-value based on the difference between the current estimate and the actual rewards received.\n",
    "\n",
    "We will represent the Q-values as a dictionary of state-action pairs, where each state is a tuple representing the current state of the board, and each action is a tuple representing the coordinates of the move. This state-action pair will be the key of our dictionary and the Q-values are the values. The initial Q-values will be set to zero. We update the action-value $Q(s_t, a_t)$ according to this formula:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha * (\\gamma * R_{t+1} - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "#### Parameters description\n",
    "- ``Q``: Dictionary with state-action pairs as key the Q-values as values.\n",
    "- ``alpha``: Learning rate.\n",
    "- ``epsilon``: Probability of doing a random move instead of the action with max Q-value.\n",
    "- ``discount_factor``: The exploration decay rate used during the training\n",
    "- ``states``: All state-action pairs a player has seen during a single match. It is used at the end of each match to update the Q-values.\n",
    "\n",
    "#### Methods description\n",
    "- ``get_Q_value``: function that returns the Q-value given a state-action pair. If the key is not present in the dictionary, it creates it with Q-value equal to 0.\n",
    "- ``add_state``: It adds to the ``states`` array a state-action pair.\n",
    "- ``reset``: It reset the ``states`` array to be able to start a new game.\n",
    "- ``choose_action``: It firstly adds to the dictionary every new state-action pair based on the new possible state of the board. It then chooses the action that can be random or based on the ``Q`` dictionary.\n",
    "- ``update_Q_value``: This function is called at every end of each game. It updates the Q-values of the ``Q`` dictionary based on the states that the player has seen during the game and the reward that they have provided.\n",
    "- ``save_policy``: It saves the ``Q`` dictionary that we have trained to a file.\n",
    "- ``load_policy``: It loads the ``Q`` dictionary from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3afea7e60b377508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T18:45:31.669394500Z",
     "start_time": "2023-12-25T18:45:31.657151200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount_factor):\n",
    "        self.Q = {}\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.states = [] # record all positions taken + action\n",
    "\n",
    "    def get_Q_value(self, state, action):\n",
    "        if (state, action) not in self.Q:\n",
    "            self.Q[(state, action)] = 0.0\n",
    "        return self.Q[(state, action)]\n",
    "    \n",
    "    def add_state(self, state, action):\n",
    "        self.states.append((state,action))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        \n",
    "    def choose_action(self, state, available_moves):\n",
    "        Q_values = [self.get_Q_value(state, action) for action in available_moves]\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_moves)\n",
    "        else:\n",
    "            max_Q = max(Q_values)\n",
    "            if Q_values.count(max_Q) > 1:\n",
    "                best_moves = [i for i in range(len(available_moves)) if Q_values[i] == max_Q]\n",
    "                i = random.choice(best_moves)\n",
    "            else:\n",
    "                i = Q_values.index(max_Q)\n",
    "            return available_moves[i]\n",
    "\n",
    "    def update_Q_value(self, reward):     \n",
    "        for st in reversed(self.states):\n",
    "            current_q_value = self.Q[(st[0], st[1])] # st[0] = board state st[1] = action\n",
    "            reward = current_q_value + self.alpha * (self.discount_factor * reward - current_q_value)\n",
    "            self.Q[(st[0], st[1])] = reward\n",
    "            \n",
    "    def save_policy(self):\n",
    "        fw = open('policy_QL', 'wb')\n",
    "        pickle.dump(self.Q, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def load_policy(self, file):\n",
    "        fr = open(file,'rb')\n",
    "        self.Q = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4419da6d7813e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train and Test functions\n",
    "---\n",
    "We used two agents that use Q-learning. They play against each other to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1898bb70a31499f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T18:45:31.670396600Z",
     "start_time": "2023-12-25T18:45:31.662664200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(player1, player2, num_episodes):\n",
    "    state = TicTacToe()\n",
    "    for epoch in tqdm(range(num_episodes)):\n",
    "        state.reset()\n",
    "        player1.reset()\n",
    "        player2.reset()\n",
    "        state_board = state.convert_matrix_to_tuple(state.board)\n",
    "        while not state.game_over:\n",
    "            #Player 1\n",
    "            action = player1.choose_action(state_board, state.available_moves())\n",
    "            player1.add_state(state_board, action)\n",
    "            state_board = state.make_move(action)\n",
    "            \n",
    "            if state.winner is not None:\n",
    "                if state.winner == 1:\n",
    "                    player1.update_Q_value(1) #player 1 won, so give 1 reward\n",
    "                    player2.update_Q_value(0)\n",
    "                elif state.winner == -1:\n",
    "                    player1.update_Q_value(0)\n",
    "                    player2.update_Q_value(1)\n",
    "                else:\n",
    "                    player1.update_Q_value(0.1) #give a less reward because we don't want ties\n",
    "                    player2.update_Q_value(0.5)\n",
    "            \n",
    "            else:\n",
    "                #Player 2\n",
    "                action = player2.choose_action(state_board, state.available_moves())\n",
    "                player2.add_state(state_board, action)\n",
    "                state_board = state.make_move(action)\n",
    "                \n",
    "                if state.winner is not None:\n",
    "                    if state.winner == 1:\n",
    "                        player1.update_Q_value(1) #player 1 won, so give 1 reward\n",
    "                        player2.update_Q_value(0)\n",
    "                    elif state.winner == -1:\n",
    "                        player1.update_Q_value(0)\n",
    "                        player2.update_Q_value(1)\n",
    "                    else:\n",
    "                        player1.update_Q_value(0.1) #give a less reward because we don't want ties\n",
    "                        player2.update_Q_value(0.5)\n",
    "    return player1, player2\n",
    "\n",
    "def test(agent, num_games, print_board=False):\n",
    "    num_wins = 0\n",
    "    num_draws = 0\n",
    "    for i in range(num_games):\n",
    "        state = TicTacToe()\n",
    "        state_board = state.convert_matrix_to_tuple(state.board)\n",
    "        while not state.game_over:\n",
    "            if state.current_player == 1:\n",
    "                action = agent.choose_action(state_board, state.available_moves())\n",
    "            else:\n",
    "                action = random.choice(state.available_moves())               \n",
    "            state_board = state.make_move(action)\n",
    "            if print_board:\n",
    "                state.show_board() \n",
    "        if state.winner == 1:\n",
    "            num_wins += 1\n",
    "        if state.winner == 0:\n",
    "            num_draws += 1\n",
    "    return num_wins, num_draws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcefe0df402f174",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hyperparameters\n",
    "---\n",
    "- ``epochs``: training epochs\n",
    "- ``alpha``: learning rate\n",
    "- ``epsilon``: probability of doing a random move instead of the action with max value\n",
    "- ``discount_factor``: the discount rate of the Bellman equation\n",
    "- ``num_games``: number of games for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2a02b30b86a76856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T18:51:20.498553900Z",
     "start_time": "2023-12-25T18:51:20.494545500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 2000000\n",
    "alpha = 0.2\n",
    "epsilon = 0.2\n",
    "discount_factor = 0.9\n",
    "num_games = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2c8da023508b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let's do some computation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45aa5abdd4dc5a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "player1 = QLearningAgent(alpha, epsilon, discount_factor)\n",
    "player2 = QLearningAgent(alpha, epsilon, discount_factor)\n",
    "\n",
    "Trained_player1, Trained_player2 = train(player1, player2, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bf3bdcf3070e89a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T18:49:42.252328800Z",
     "start_time": "2023-12-25T18:49:42.245683200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "\n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "\n",
      "-------------\n",
      "| o | o | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "\n",
      "-------------\n",
      "| o | o | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# Trainer_player1 is the X\n",
    "_ = test(agent=Trained_player1, num_games=1, print_board=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ebaea50149e4d934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T18:55:47.551404700Z",
     "start_time": "2023-12-25T18:55:47.456528300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 911 wins, 47 losses, 42 draws\n",
      "Wins + Draws percentage: 95.3\n"
     ]
    }
   ],
   "source": [
    "num_wins, num_draws = test(agent=Trained_player1, num_games=num_games)\n",
    "print(f\"Over 1000 matches: {num_wins} wins, {1000 - num_wins - num_draws} losses, {num_draws} draws\")\n",
    "print(f\"Wins + Draws percentage: {(num_wins + num_draws) / num_games * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
