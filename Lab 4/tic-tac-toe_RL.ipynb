{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tic Tac Toe\n",
    "---\n",
    "\n",
    "<img style=\"float:center\" src=\"../images/tris.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T13:32:47.699048800Z",
     "start_time": "2023-12-24T13:32:47.561809100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Board State\n",
    "---\n",
    "The ``TicTacToe`` class reflects the state of the board.\n",
    "We use 1 to indicate player1 and -1 for player 2.\n",
    "\n",
    "#### Parameter description\n",
    "- ``board``: numpy array of dimension 3x3 that represents the game board.\n",
    "- ``p1``: player1 class.\n",
    "- ``p2``: player2 class.\n",
    "- ``current_player``: indicates who has to take the turn.\n",
    "- ``isEnd``: boolean that indicate if the game has finished.\n",
    "- ``boardHash``: it is the board status but as a string.\n",
    "\n",
    "#### Methods description\n",
    "- ``available_positions``: it returns an array with the list of possible moves (each element is a tuple of two integers that indicates where to play).\n",
    "- ``make_move``: takes as input the location of where a player played and puts the value of the ``current player`` in that place on the board, i.e. who is playing at that moment. It also gives the other player the turn by changing the ``current_player`` parameter.\n",
    "- ``get_hash``: it returns the board state but in a string format.\n",
    "- ``check_winner``: It checks if there is a winner. \n",
    "- ``reward``: It calls the function of the players that update the value estimation of states giving them the reward (1 if a player won, 0 if he loses). \n",
    "- ``reset``: It resets the state of the board by emptying the boxes with also all the other parameters.\n",
    "- ``show_board``: It prints the board status. ``player1`` is the X and ``player2`` is the O.\n",
    "- ``train``: We used two agents that use Reinforcement Learning to play against each other. During training the process of each player is: look for available positions, choose action, update board state and add the action to player's states, judge if reach the end of the game and give reward accordingly.\n",
    "- ``test``: We test our trained policy with a random player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T19:03:36.059780600Z",
     "start_time": "2023-12-24T19:03:36.055754200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self,p1,p2):\n",
    "        self.board = np.zeros((3,3))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        self.current_player = 1 #1 is p1, -1 is p2\n",
    "\n",
    "    def available_positions(self):\n",
    "        pos = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i,j] == 0:\n",
    "                    pos.append((i,j))\n",
    "        return pos\n",
    "    \n",
    "    def make_move(self, position):\n",
    "        if position not in self.available_positions():\n",
    "            return None\n",
    "        self.board[position] = self.current_player\n",
    "        self.current_player = self.current_player*-1\n",
    "\n",
    "    def get_hash(self):\n",
    "        self.boardHash = str(self.board.reshape(3 * 3))\n",
    "        return self.boardHash\n",
    "\n",
    "    def check_winner(self):\n",
    "        #check if rows contains 3 or -3 (someone win)\n",
    "        for i in range(3): \n",
    "            if sum(self.board[i,:]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1 #player 1 won\n",
    "        for i in range(3): #loop on the rows\n",
    "            if sum(self.board[i,:]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1 #player 2 won\n",
    "        \n",
    "        #check if col contains 3 or -3\n",
    "        for i in range(3):\n",
    "            if sum(self.board[:,i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "        for i in range(3):\n",
    "            if sum(self.board[:,i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        \n",
    "        #check diagonal win\n",
    "        diag_sum = sum([self.board[i,i] for i in range(3)])\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd= True\n",
    "            return 1\n",
    "        if diag_sum == -3:\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        diag_sum = sum([self.board[i,3-i-1] for i in range(3)])\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd= True\n",
    "            return 1\n",
    "        if diag_sum == -3:\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        #here no one won..\n",
    "        if len(self.available_positions())==0 :\n",
    "            self.isEnd = True\n",
    "            return 0 #no one won\n",
    "        \n",
    "        return None #Here there are still moves, so keep playing !!!\n",
    "    \n",
    "    def reward(self, result):\n",
    "        if result == 1:\n",
    "            self.p1.give_rew(1) #player 1 won, so give 1 reward\n",
    "            self.p2.give_rew(0)\n",
    "        elif result == -1:\n",
    "            self.p1.give_rew(0)\n",
    "            self.p2.give_rew(1)\n",
    "        else:\n",
    "            self.p1.give_rew(0.1) #give a less reward because we don't want ties\n",
    "            self.p2.give_rew(0.5)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.current_player = 1\n",
    "\n",
    "    def show_board(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, 3):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, 3):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')    \n",
    "\n",
    "    def train(self, rounds=10000):\n",
    "        for epochs in tqdm(range(rounds)):\n",
    "            while not self.isEnd:\n",
    "                \n",
    "                # Player 1\n",
    "                positions = self.available_positions()\n",
    "                p1_action = self.p1.choose_action(positions, self.board, self.current_player)\n",
    "                # take action and update board state\n",
    "                self.make_move(p1_action)\n",
    "                board_hash = self.get_hash()\n",
    "                self.p1.add_state(board_hash)\n",
    "                # check the board status if it is ended\n",
    "                win = self.check_winner()\n",
    "                \n",
    "                if win is not None: #It returns None only when no one finished or tied.\n",
    "                    # self.showBoard()\n",
    "                    # ended with p1 either win or draw\n",
    "                    self.reward(win) #send rewards to the players, the game has ended\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.available_positions()\n",
    "                    p2_action = self.p2.choose_action(positions, self.board, self.current_player)\n",
    "                    self.make_move(p2_action)\n",
    "                    board_hash = self.get_hash()\n",
    "                    self.p2.add_state(board_hash)\n",
    "\n",
    "                    win = self.check_winner()\n",
    "                    if win is not None:\n",
    "                        # self.showBoard()\n",
    "                        # ended with p2 either win or draw\n",
    "                        self.reward(win)\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "\n",
    "    def test(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.available_positions()\n",
    "            p1_action = self.p1.choose_action(positions, self.board, self.current_player)\n",
    "            # take action and update board state\n",
    "            self.make_move(p1_action)\n",
    "            # check board status if it is ended\n",
    "            win = self.check_winner()\n",
    "            if win is not None: #if win is not None means someone win or tie\n",
    "                return win\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.available_positions()\n",
    "                p2_action = self.p2.choose_action(positions, self.board, self.current_player)\n",
    "\n",
    "                self.make_move(p2_action)\n",
    "                win = self.check_winner()\n",
    "                if win is not None:\n",
    "                    return win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reinforcement Learning Player\n",
    "---\n",
    "\n",
    "This class represents a player that uses Reinforcement Learning to make decisions in Quixo. More precisely, our player uses [Temporal difference (TD) learning](https://it.wikipedia.org/wiki/Temporal_difference_learning). TD in reinforcement learning is an unsupervised learning technique very commonly used in it for the purpose of predicting the total reward expected over the future. Essentially, TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods perform state value function updates based on current estimates.\n",
    "\n",
    "#### Parameters description\n",
    "- ``states_value``: Dictionary that has as key the states that a player has seen during all the matches and as value the parameter that we want to train.\n",
    "- ``exp_rate``: Probability of doing a random move instead of the action with max Q-value.\n",
    "- ``decay_gamma``: The exploration decay rate used during the training\n",
    "- ``states``: All state-action pairs a player has seen during a single match. It is used at the end of each match to update the ``states_value``.\n",
    "\n",
    "#### Methods description\n",
    "- ``get_hash``: it returns the board state but in a string format.\n",
    "- ``add_state``: It adds to the ``states`` array the state that a player has seen during a game.\n",
    "- ``reset``: It reset the ``states`` array to be able to start a new game.\n",
    "- ``choose_action``: It receives as input all the possible ``positions`` to play, the ``current_board`` that is the status of the board and the ``simbol`` that indicate the current player (1 for player1 and -1 for player2). This function has the job to decide the move of a player that can be random or based on the value of the dictionary. It takes from the dictionary, for each possible move, the value associated with the state of the board with the move performed. The maximum value will be the move to execute. We use the following recursive (bellman equation) formula to compute the state-value table: \n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha * (\\gamma * V(S_t +1) - V(S_t))\n",
    "$$\n",
    "The formula simply tells us that the updated value of state t equals the current value of state t adding the difference between the value of the next state , which is multiplied by the discount factor of the Bellman Equation, and the value of the current state, which is multiplied by a learning rate α. The logic is that we update the current value slowly based on our latest observation.\n",
    "- ``give_rew``: This function is called at every end of each game. It updates the values of the ``states_value`` dictionary based on the states that the player has seen during the game and the reward that they have provided.\n",
    "- ``save_policy``: It saves the ``states_value`` dictionary that we have trained to a file.\n",
    "- ``load_policy``: It loads the ``states_value`` dictionary from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T13:32:08.164996400Z",
     "start_time": "2023-12-24T13:32:08.156480800Z"
    }
   },
   "outputs": [],
   "source": [
    "class RLPlayer:\n",
    "    def __init__(self, name, lr=0.2, decay_gamma=0.9, exp_rate = 0.2):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken\n",
    "        self.lr = lr\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = decay_gamma\n",
    "        self.states_value = {}  # state -> value\n",
    "\n",
    "    def get_hash(self, board):\n",
    "        boardHash = str(board.reshape(3*3))\n",
    "        return boardHash\n",
    "\n",
    "    def add_state(self, state):\n",
    "        self.states.append(state)\n",
    "\n",
    "    def choose_action(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate: # Do exploration, take random \n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else: #Here do exploitation, take the action that has the highest value\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy() #create a tmp board\n",
    "                next_board[p] = symbol #do the action\n",
    "                next_board_hash = self.get_hash(next_board) #get the hash\n",
    "                value = 0 if self.states_value.get(next_board_hash) is None else self.states_value.get(next_board_hash)\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max: #find the action that has max value. \n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    def give_rew(self, reward):\n",
    "        #At the end of the game, I'll get a reward. The iterating on the states in reverse.\n",
    "        # Set the value of the state to 0 if not existing, otherwise update it with the reward. \n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None: #if the state doesn't have a value, set it to 0\n",
    "                self.states_value[st] = 0\n",
    "            #this is V(t) = V(t) + lr * (gamma*V(t+1) - V(t))\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "            \n",
    "    def save_policy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def load_policy(self, file):\n",
    "        fr = open(file,'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T13:33:02.006542600Z",
     "start_time": "2023-12-24T13:33:01.996018Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = \"random\"\n",
    "\n",
    "    def choose_action(self, positions, board, current_player):\n",
    "        x = np.random.randint(0,len(positions)-1)\n",
    "        return positions[x]\n",
    "    \n",
    "    def add_state(self, state):\n",
    "        pass\n",
    "\n",
    "    def give_rew(self, reward):\n",
    "        pass\n",
    "            \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hyperparameters\n",
    "---\n",
    "- ``epochs``: training epochs\n",
    "- ``alpha``: learning rate\n",
    "- ``epsilon``: probability of doing a random move instead of the action with max value\n",
    "- ``discount_factor``: the discount rate of the Bellman equation\n",
    "- ``num_games``: number of games for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T22:01:48.678894Z",
     "start_time": "2023-12-24T22:01:48.675947100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 50000\n",
    "alpha = 0.2\n",
    "epsilon = 0.2\n",
    "discount_factor = 0.9\n",
    "num_games = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T13:32:27.946659900Z",
     "start_time": "2023-12-24T13:32:27.940328Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1 = RLPlayer(\"p1_RL\", lr=alpha, decay_gamma=discount_factor, exp_rate=epsilon)\n",
    "p2 = RLPlayer(\"p2_RL\", lr=alpha, decay_gamma=discount_factor, exp_rate=epsilon)\n",
    "st = TicTacToe(p1, p2)\n",
    "\n",
    "print(\"training...\")\n",
    "st.train(rounds=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T16:27:02.253898300Z",
     "start_time": "2023-12-24T16:27:01.497538Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 1000 matches: 910 wins, 51 losses, 39 draws\n",
      "Wins + Draws percentage: 94.89999999999999\n"
     ]
    }
   ],
   "source": [
    "p2 = RandomPlayer(\"Random\")\n",
    "st = TicTacToe(p1,p2)\n",
    "win_comp = 0\n",
    "num_draws = 0\n",
    "\n",
    "for epoch in range(num_games):\n",
    "    win = st.test()\n",
    "    if win == 1:\n",
    "        win_comp+=1\n",
    "    if win == 0:\n",
    "        num_draws+=1\n",
    "    st.reset()\n",
    "\n",
    "print(f\"Over 1000 matches: {win_comp} wins, {1000 - win_comp - num_draws} losses, {num_draws} draws\")\n",
    "print(f\"Wins + Draws percentage: {(win_comp + num_draws) / epochs * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
